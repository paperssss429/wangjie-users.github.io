<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[机器学习中的线性代数知识（中）]]></title>
    <url>%2F2018%2F11%2F13%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E7%BA%BF%E4%BB%A3%E7%9F%A5%E8%AF%86%E4%B8%8B%2F</url>
    <content type="text"><![CDATA[前言在机器学习中的线性代数知识（上）一文中，主要讲解了矩阵的本质，以及映射视角下的特征值与特征向量的物理意义。本篇博文将继续讨论相关知识。 向量和矩阵的范数归纳1、向量的范数这里我直接以实例讲解。定义一个向量为：a=[-5, 6, 8, -10]向量的1范数：向量的各个元素的绝对值之和，上述向量a的1范数结果就是：29；向量的2范数：向量的每个元素的平方和再开平方根，上述a的2范数结果就是：15；向量的负无穷范数：向量的所有元素的绝对值中最小的。上述向量a的负无穷范数结果就是：5；向量的正无穷范数：向量的所有元素的绝对值中最大的。上述向量a的正无穷范数结果就是：10； 2、矩阵的范数定义一个矩阵A=[-1 2 -3；4 -6 6]；矩阵的1范数：矩阵每一列上的元素绝对值先求和，再从中取最大值（列和最大）。上述矩阵A的1范数就是9；矩阵的2范数：矩阵$A^TA$的最大特征值开平方根，上述矩阵A的2范数就是：10.0623；矩阵的无穷范数：矩阵的每一行上的元素绝对值先求和，再从中取个最大的(行和最大)，上述矩阵的结果就是：16；矩阵的核范数：矩阵的奇异值之和(将矩阵SVD分解，文章后面会讲到)，这个范数可以用低秩表示（因为最小化核范数，相当于最小化矩阵的秩——低秩），上述矩阵A最终结果就是：10.9287；矩阵的L0范数：矩阵的非0元素的个数，通常用它来表示稀疏性，L0范数越小0元素越多，也就越稀疏，上述矩阵A的最终结果就是：6；矩阵的L1范数：矩阵中的每个元素绝对值之和，它是L0范数的最优凸近似，因此它也可以表示稀疏，上述矩阵A最终结果就是：22；矩阵的F范数：矩阵的各个元素平方之和再开平方根，它通常也叫做矩阵的L2范数。上述矩阵A的结果是：10 深入理解二范数二范数相信大家在本科学线代的时候就已经被灌输了“用来度量向量长度”、“用来度量向量空间中两个点的距离”这两个典型意义。那么在机器学习中，二范数主要有什么重要的应用呢？ 我们经常在机器学习的loss函数中加上参数的2范数项，以减少模型对训练集的过拟合，即提高模型的泛化能力。那么问题来了，为什么2范数可以提高模型的泛化能力呢？ 首先假设我们model的参数为向量W=[W1, W2, …, Wn], 这个w不仅是模型的参数，更是特征的权重！。换句话说，每个参数Wi，决定了每个特征对决定样本所属类别的重要程度。 这里我们举一个实际例子，假设参数w的维度为5，有以下两种情况： 令W1=W2=W3=W4=W5=2 令W1=10, W2=W3=W4=W5=0 由二范数公式：1和2相比，哪个的Reg更小呢？显然前者的值只有20，而后者的值高达100！虽然1和2的所有w的值加起来都等于10。而我们的目标是最小化loss函数，因此需要Reg也要小。由这个例子可以看出，如果我们有10张用于决定类别的票分给各个特征，那么给每个特征分两张票带来的回报要远大于把这10张票分给一个特征！（不患寡而患不均），因此二范数会削弱强特征，增强弱特征。（有点儿劫富济贫的意思） 那么这种方式有什么好处呢？ 比如，假设我们在做文本的情感分类任务（判断一段文本是正面还是负面），将每个词作为一个特征（出现该词代表1，否则为0）。可想而知，有一些词本身就带有很强的情感极性，比如“不好”、“惊喜”等。而大部分词是弱极性的，但是多个弱极性的词同时出现的时候就会产生很强的情感极性。比如“总体”“来说”“还是”“可以”在文本中同时出现后基本就奠定了这篇文本的总体极性是正面的，哪怕文本中出现了“很烂”这种强负面词。 因此在二范数的约束下，w这个随机变量的分布会趋向于方差u=0的高斯分布。当然，既然是高斯分布，有极少的特征值是特别大的，这些特征就是超强特征，比如“非常好”这个特征一旦出现，基本整个文本的情感极性就确定了，其它的弱特征是很难与之对抗的。所以最小化二范数会让随机变量的采样点组成的向量趋向于期望=0的高斯分布。 所以，如果没有二范数的约束，弱特征会被强特征“剥削”，最终训练完后各个特征的权重很有可能是这样的：这样会带来什么问题呢？导致模型过分依赖强特征。 首先，这样的model拿到测试集上，一旦某个样本没有任何强特征，就会导致这个样本的分类很随机。而如果有了二范数的约束，弱特征们就会发挥它们的作用。 其次，这样的model抗噪声能力非常差。一旦测试集中的某个样本出现了一个强特征，就会直接导致其进行分类，而完全不考虑其它弱特征的存在。而如果有了二范数的约束，弱特征们就会聚沙成塔，合力打倒那个强特征的噪声。那么是不是所有的ML任务加上二范数约束就一定好呢？答案当然是否定的。在一些ML任务中，尤其是一些结构化数据挖掘任务和特征意义模糊的机器学习任务（比如深度学习）中，特征分布本来就是若干强特征与噪声的组合，这时加上2范数约束反而会引入噪声，降低系统的抗噪性能，导致model的泛化能力更差。因此，使用二范数去提高机器学习model的泛化能力大部分情况下是没错的，但是也不要无脑使用哦，懂得意义后学会根据任务去感性与理性的分析才是正解。这里有一篇分析机器学习中正则化项L1和L2的直观理解，大家可以借鉴阅读。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习中的线性代数知识（上）]]></title>
    <url>%2F2018%2F11%2F08%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E7%9F%A5%E8%AF%86(%E4%B8%8A)%2F</url>
    <content type="text"><![CDATA[前言As we all know, 线性代数对于机器学习的重要性不言而喻。但纵观国内的教材和课程，大部分线性代数的讲解，一上来就堆满了各种定义和公式，从而导致我们知其然而不知其所以然，不利于我们深入理解机器学习的算法。因此，希望本篇博文能帮大家从另一个角度理解线性代数。但是注意，阅读本篇博文，最好已经有了线性代数的基础，比如要知道矩阵的形式，矩阵怎么相乘之类的基础知识。然后如果你虽然知道矩阵相乘怎么算，却并不知道它们有什么物理含义，那么相信本篇博文会让您有所收获。本系列博文分为上下两篇，上篇主要讲矩阵的本质，下篇主要讲向量的范数和矩阵分解。 矩阵的本质（一）1、矩阵本质上就是映射、变换比如，对于空间A中的一个三维点a(a1,a2,a3)，空间B的一个二维点b(b1,b2)，那么如何将a点映射成b点呢？只需构造一个维度为3*2的矩阵，利用矩阵乘法便可实现映射。矩阵的本质就是描述空间的映射。因此它可以看作一个函数，接受一个空间作为输入，输出一个映射后的新空间。 2、各种矩阵运算的本质是什么？ 加法：矩阵的加法就是映射规则的调整。矩阵A+矩阵B就可以看作，用B中各个元素的值去调整A这个映射规则。 乘法：矩阵的乘法表示线性映射的叠加。比如，我们已经知道矩阵A可以从X空间映射到Z空间，矩阵B可以从Z空间映射到Y空间，那么怎么从X空间映射到Y呢？答案就是矩阵乘法A*B。 逆运算：矩阵的逆运算就是映射的逆映射；这里我用一个NLP的例子来解释，比如在一个机器翻译问题中，源语言在X空间，目标语言在Y空间，我们通过训练知道了如何从X空间映射到Y空间，即通过训练得到了映射矩阵A。那么我们如何反向翻译呢？那就是求A的逆矩阵就可以了，完全无需再反向重新训练。 3、特殊矩阵的含义（从映射角度理解） 零矩阵就是将任何空间映射成原点的矩阵； 单位矩阵就是将任何空间映射成自己的矩阵； 对角矩阵就是将原空间的各个坐标轴进行不同尺度的拉伸或挤压后生成新空间的矩阵；比如任何一个二维空间乘上一个对角矩阵后，这个空间第一个维度（每一列表示一个维度）上的坐标轴会缩短为原来的一半，第二个维度的坐标轴会膨胀成原来的两倍！假设这个二维空间中的点(2,2)经过这个对角矩阵的映射后，会变成(1,4)。4、矩阵分块的意义矩阵分块对应的算法思想就是分治，一个最重要的应用就是并行化矩阵运算，这也是深度学习中最离不开的底层运算。设想一下，一个10000*10000维的超大矩阵再乘以另一个10000*20000维的超大矩阵，如果让一个单核CPU去计算的话，可能CPU会崩溃！我们知道，在深度学习任务中，由于参数非常多，数据维度也很高，因此进行大矩阵运算是家常便饭。同时，我们之所以喜欢用GPU去训练深度学习任务，一个很重要的原因是GPU的“核”很多，经常几百几千，甚至几万核，非常利于并行计算。因此，在深度学习中，通过将大矩阵分块，分成若干个小块，再将这些小块丢进GPU成千上万的核里并行计算，那么转瞬间就可以完成这个大矩阵的计算。矩阵的本质（二）1、从存储数据的角度矩阵A就是存储数据的作用了。矩阵的每一行表示空间中的一个样本点，所以A就表示二维空间里的三个样本点；如果将A这三个样本点丢给映射W，就得到了三个样本点在新空间上的新空间上的镜像点A x W =在神经网络中，每一层的输出经过权重矩阵，映射到下一层的输入过程，就是上述过程。2、特征值与特征向量的物理意义我们将矩阵W重新夸张一下W=那么这个映射就代表将第一维度压缩为原来的0.99倍（几乎无变化），将第二维度拉伸为原来的100倍（变化很大）。这个映射的作用对象很明显： 第一维度坐标轴。在二维空间中，第一维度坐标轴不就是x轴嘛，所以我们可以用单位向量(1, 0)表示； 第二维度坐标轴。同理，第二维度可表示为(0, 1);显而易见，W映射对每个作用对象的作用程度明显不一样：对第一维度作用小，对第二维度作用大；升华一下，上述的作用对象就是所谓的特征向量，“对某作用对象的作用程度” 就是特征值。因此，一个矩阵或一个线性映射，完全可以用它的全部特征值与其对应的特征值去描述！（这里指方阵）；而将矩阵分解为它的特征值与特征向量的过程被称为“特征分解”，特征矩阵分解是众多矩阵分解中的一种，因此原矩阵A就会被分解成几个目标矩阵了，这里的目标矩阵有两个，一个是由特征向量组成的，另一个是由特征值组成的；3、标量、向量、张量之间的联系 张量（tensor）在某些情况下，我们会讨论坐标超过两维的数组。一般地，一个数组中的元素分布在若干维坐标的规则网格中，我们将其称之为张量。使用粗体A来表示张量“A”。张量A中坐标为(i,j,k)的元素记作A(i,j,k)。 关系标量是0阶张量，向量是一阶张量。 举例：标量就是知道棍子的长度，但是不知道棍子指向哪儿；向量就是不但知道棍子的长度，还知道棍子指向前面还是后面；张量就是不但知道棍子的长度，也知道棍子指向前面还是后面，还能知道这棍子向上/下和左/右偏转了多少。4、张量与矩阵的区别 从代数角度讲，矩阵是向量的推广。向量可以看成是一维的“表格”（即分量按照顺序排成一列），矩阵是二维的“表格”（分量按照纵横位置排列），那么n阶张量就是所谓的n维的“表格”，故可以将矩阵视为二阶张量。张量的严格定义是利用线性映射来描述的。 从几何角度讲，矩阵是一个真正的几何量，也就是说，它是一个不随参照系的坐标变换而变化的东西。向量也具有这种特性。 矩阵是张量的一个切片。 张量是矩阵基础上更高维度的抽象，简而言之，张量囊括一切。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法的评价指标]]></title>
    <url>%2F2018%2F11%2F06%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%2F</url>
    <content type="text"><![CDATA[前言机器学习的目标是使得我们模型的泛化能力足够强，因此我们需要有衡量模型泛化能力的评价标准。对不同种类的机器学习问题，评价标准不一样。回归问题常用均方误差(MSE)、绝对误差(MAE)等评价指标，分类问题评价指标则较多，如下图所示。本文主要讲解分类问题的评价指标。 二分类问题中常用的概念首先解释几个二分类问题中常用的概念：True Positive, False Positive, True Negative, False Negative。它们是根据真实类别与预测类别的组合来区分的。假设有一批测试样本，只包含正例和反例两种类别。则有： 预测值正例，记为P(positive) 预测值为反例，记为N(Negative) 预测值与真实值相同，记为T(True) 预测值与真实值相反，记为F(False)上图左半部分表示预测为正例，右半部分预测类别为反例；样本中的真实正例类别在上半部分，下半部分为真实的反例。 TP: 预测类别为正，真实类别也为正例； FP：预测类别为正例，真实类别为反例； FN：预测为反例，真实类别为正例； TN：预测为反例，真实类别也为正例； 精确率（precision）精确率是针对我们模型预测的结果而言的，它表示的是预测为正的样本中有多少是真正的正样本；故分子为TP，而预测为正有两种可能：真正(TP)和假正(FP)，即为分母。 召回率（recall）召回率是针对原始样本而言的，表示样本中的正例有多少被预测正确了，故分母应表示所有的原始样本； F1值F1值是精确率和召回率的调和均值，当P和R值都高时，F1值也会高。 ROC、AUCROC全称是“受试者工作特征”（Receiver Operating Characteristic）。对于计算ROC，有三个重要概念：TPR，FPR，截断点。TPR即 True Positive Rate，表示真实的正例中，被预测正确的比例，即TPR = TP/(TP+FN);FPR即 False Positive Rate, 表示真实的反例中，被预测正确的比例，即FPR = FP/(TN+FP);很多ML算法是为test样本产生一个概率预测，然后将这个概率预测值与一个分类阈值(threhold)进行比较，若大于阈值则分为正类，反之为负类。这里的阈值就是截断点，比如对算出测试样本A为正类的概率为0.3，低于截断点0.5，于是将A分为负类。截断点取不同的值，TPR和FPR的计算结果也不同。将截断点不同取值下对应的TPR和FPR结果作图得到的曲线，就是ROC曲线。横轴是FPR，纵轴是TPR。若比较两个算法的优劣，可以看哪个曲线能把另一个完全包住；或者计算曲线的面积，即AUC的值。ROC经常用于样本类别不均衡的二分类问题。 sklearn实现roc、auc123from sklearn.metrics import roc_curve, aucfpr, tpr, th = roc_curve(test_Y, pred_y) # 得到一组fpr、tpr的值roc_auc = auc(fpr, tpr) # 计算auc的值]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
</search>
