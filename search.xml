<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[深入理解L1、L2范数]]></title>
    <url>%2F2018%2F11%2F28%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3L1%E3%80%81L2%E8%8C%83%E6%95%B0%2F</url>
    <content type="text"><![CDATA[前言说起L1、L2范数，大家会立马想到这是机器学习中常用的正则化方法，一般添加在损失函数后面，可以看作是损失函数的惩罚项。那添加L1和L2正则化后到底有什么具体作用呢？为什么会产生这样的作用？本篇博文将和大家一起去探讨L1范数、L2范数背后的原理。 先说结论L1和L2的作用如下： L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择；一定程度上可以防止过拟合 L2正则化可以防止模型过拟合理解L1范数理解L1，主要需要理解两个问题。第一是L1产生稀疏矩阵的作用，第二是为什么L1可以产生稀疏模型。稀疏模型与特征选择稀疏矩阵指的是很多元素为0、只有少数元素是非零值的矩阵。以线性回归为例，即得到的线性回归模型的大部分系数都是0，这表示只有少数特征对这个模型有贡献，从而实现了特征选择。总而言之，稀疏模型有助于进行特征选择。为什么L1正则化能产生稀疏模型？这部分重点讨论为什么L1可以产生稀疏模型，即L1是怎么让系数等于0的。首先要从目标函数讲起，假设带有L1正则化的损失函数如下：其中J0是损失函数，后边是L1正则化项，$\alpha$是正则化系数，$\omega$是模型的参数。现在我们的目标是求解argmin$\omega$(J)，换句话说，我们的任务是在L1的约束下求出J0取最小值的解。假设只考虑二维的情况，即只有两个权值$\omega$1和$\omega$2，此时的L1正则化公式即为：L1 = |$\omega$1| + |$\omega$2|。对J使用梯度下降法求解，则求解J0的过程可以画出等值线，同时L1正则化的函数也可以在二维平面上画出来。如下图：图1 L1正则化 图中等值线是J0的等值线，黑色方形是L1函数的图形，J0等值线与L1图形首次相交的地方就是最优解，我们很容易发现黑色方形必然首先与等值线相交于方形顶点处。可以直观想象，因为L1函数有很多”突出的角“（二维情况下有四个，多维情况下更多），J0与这些角接触的概率远大于与其它部分接触的概率。而这些点某些维度为0（以上图为例，交点处$\omega$1为0），从而会使部分特征等于0，产生稀疏模型，进而可以用于特征选择。 理解L2范数为什么L2范数可以防止过拟合呢？要想知道L2范数为什么可以防止过拟合，首先就要知道什么是过拟合。通俗讲，过拟合是指模型参数较大，模型过于复杂，模型抗扰动能力弱。只要测试数据偏移一点点，就会对结果造成很大的影响。因此，要防止过拟合，其中一种方法就是让参数尽可能的小一些。同L1范数分析一样，我们做出图像，如下图所示：图2 L2正则化二维平面下L2正则化的函数图形是个圆，与方形相比，没有突出的棱角。因此交点在坐标轴的概率很低，即使得$\omega$1或$\omega$2等于零的概率小了许多。由上图可知，L2中得到的两个权值倾向于均为非零的较小数。 这也就是L1稀疏、L2平滑的原因。 下面我从公式的角度解释一下，为什么L2正则化可以获得值很小的参数？ 以线性回归中的梯度下降法为例。假设要求的参数为$\theta$, h$\theta$(x)是我们的model，那么LR的损失函数如下：那么在梯度下降法中，最终用于迭代计算参数$\theta$的迭代式为：当对损失函数加上L2正则化以后，迭代公式会变成下面的样子:从上式可以看出，与未添加L2正则化的迭代公式相比，每一次迭代，$\theta$j都要乘以一个小于1的因子，从而使得$\theta$j不断减小，因此总的来看，$\theta$是不断减小的。 总结L1会趋向于产生少量的特征，而其它特征都是0。L2会选择更多的特征，这些特征都会趋近于0。L1在特征选择时非常有用，而L2只是一种防止过拟合的方法。在所有特征中只有少数特征起重要作用的情况下，选择L1范数比较合适，因为它能自动选择特征。而如果所有特征中，大部分特征都能起作用，而且起的作用很平均，那么使用L2范数也许更合适。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习、深度学习中常用的Linux命令]]></title>
    <url>%2F2018%2F11%2F19%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%81%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84Linux%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[前言作为一名coder，Linux命令是基本生存技能，但Linux的命令何其多，要想完全掌握可不易。。。在我平常的学习和工作中，Linux服务器主要是用来跑ML、DL代码的（Linux运维大神请略过作为一名coder，Linux命令是基本生存技能，但Linux的命令何其多，要想完全掌握可不易。。。在我平常的学习和工作中，Linux服务器主要是用来跑ML、DL代码的（Linux运维大神请略过…），因此本文将主要从跑代码的角度浅谈一下常用的Linux操作。 基本操作1、常用快捷键12345// 告别手指抽筋的方向键 ctrl + e // 光标回到行末 ctrl + k // 删除光标处到行尾的字符 ctrl + u // 删除命令行的整段命令 ctrl + y // 恢复上一次删除内容 123456// vim快捷键（命令行模式下使用） gg: 将光标移动到文档开头 G: 将光标移动到文档末尾 $: 将光标移动到本行尾 0: 将光标移动到本行行首 ndd: 删除n行(如10+dd) 2、文件夹操作12345pwd //显示当前路径ls //显示当前目录的内容ll //详细显示内容du -sh * //显示当前文件夹下所有内容的大小tar -xzvf //解压命令 3、复制、移动、重命名、删除12345cp source destination //将目标赋值到目的地mv source desination //将目标移动到目的地mv file1 file2 //将file1重名为file2rm //删除文件，慎用rm -f rm -r //删除文件夹 4、查询、检索、统计12find . -name "*.py" | wc -l //查看当前文件夹（包含子文件夹）下共有多少个py文件grep -o root a.txt | wc -l // 统计文件a.txt中root这个词出现的次数 5、shell脚本、txt文件等的操作1234chmod +x run.sh //更改shell脚本的权限./run.sh //运行shell脚本，也可用 sh run.shcat test.txt //查看test.txt文件wc -l test.txt //显示test.txt文件有多少行 进阶操作1、如何让代码在后台运行由于实验室的网络有时候不稳定，会导致跑了好几个小时的代码在快要出结果的时候断网了，导致与服务器的连接中断，从而代码也就停止运行了，这点着实让人很苦恼。同时，如果在跑代码的时候还想进行其它Linux操作，也可以考虑把代码放到服务器后台运行。这里主要有两种方法实现代码的后台运行。 第一种1nohup python model.py &gt;log.txt 2&gt;&amp;1 &amp; //将代码后台挂起运行，并将结果输出到log.txt中，2&gt;&amp;1表示若有错误也将一同输出到log.txt中 第二种如果我想实时的观察到代码的运行状态，并不想最终运行完以后才能在日志文件中看。这时可以用screen这个命令。可以简单的认为用这个命令你可以为不同的任务开不同的窗口，这个窗口之间是可以切换的，同时，窗口和你的会话连接基本上没有任何区别，这样你可以在开一个连接的时候同时干多件事情，并且在终端看得到运行过程的同时而不会由于断网而导致代码停止运行。常用命令如下：1screen -S train_cnn //创建一个新窗口并命名为train_cnn 当你执行完上述命令后，就会自动跳入名为train_cnn的窗口中，在这个窗口里你可以开始跑代码。然后通过快捷键ctrl + a + d 断开这个窗口的连接而回到会话界面，注意这里只是断开了窗口并未终止任务的运行。 可以通过上述命令开启多个窗口。可通过以下命令查看所有的窗口，并连接到其中一个窗口12screen -ls //显示所有窗口screen -r train_cnn //返回到train_cnn窗口下 如果要断开某个窗口，可直接使用快捷键 ctrl + d 退出即可。 总结一下，screen可以实现代码在后台运行时的可视化，同时，能在开一个会话连接时创建多个窗口处理不同的任务。用起来也很方便。 2、服务器上jupyter notebook的使用在服务器上jupyter notebook也是经常需要使用到的。服务器下配置jupyter(已安装anaconda)可以参考这篇文章。]]></content>
      <categories>
        <category>Linux命令</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习中的线性代数知识（中）]]></title>
    <url>%2F2018%2F11%2F13%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E7%9F%A5%E8%AF%86%EF%BC%88%E4%B8%AD%EF%BC%89%2F</url>
    <content type="text"><![CDATA[前言在机器学习中的线性代数知识（上）一文中，主要讲解了矩阵的本质，以及映射视角下的特征值与特征向量的物理意义。本篇博文将继续讨论相关知识。 向量和矩阵的范数归纳1、向量的范数这里我直接以实例讲解。定义一个向量为：a=[-5, 6, 8, -10]向量的1范数：向量的各个元素的绝对值之和，上述向量a的1范数结果就是：29；向量的2范数：向量的每个元素的平方和再开平方根，上述a的2范数结果就是：15；向量的负无穷范数：向量的所有元素的绝对值中最小的。上述向量a的负无穷范数结果就是：5；向量的正无穷范数：向量的所有元素的绝对值中最大的。上述向量a的正无穷范数结果就是：10； 2、矩阵的范数定义一个矩阵A=[-1 2 -3；4 -6 6]；矩阵的1范数：矩阵每一列上的元素绝对值先求和，再从中取最大值（列和最大）。上述矩阵A的1范数就是9；矩阵的2范数：矩阵$A^TA$的最大特征值开平方根，上述矩阵A的2范数就是：10.0623；矩阵的无穷范数：矩阵的每一行上的元素绝对值先求和，再从中取个最大的(行和最大)，上述矩阵的结果就是：16；矩阵的核范数：矩阵的奇异值之和(将矩阵SVD分解，文章后面会讲到)，这个范数可以用低秩表示（因为最小化核范数，相当于最小化矩阵的秩——低秩），上述矩阵A最终结果就是：10.9287；矩阵的L0范数：矩阵的非0元素的个数，通常用它来表示稀疏性，L0范数越小0元素越多，也就越稀疏，上述矩阵A的最终结果就是：6；矩阵的L1范数：矩阵中的每个元素绝对值之和，它是L0范数的最优凸近似，因此它也可以表示稀疏，上述矩阵A最终结果就是：22；矩阵的F范数：矩阵的各个元素平方之和再开平方根，它通常也叫做矩阵的L2范数。上述矩阵A的结果是：10 深入理解二范数二范数相信大家在本科学线代的时候就已经被灌输了“用来度量向量长度”、“用来度量向量空间中两个点的距离”这两个典型意义。那么在机器学习中，二范数主要有什么重要的应用呢？ 我们经常在机器学习的loss函数中加上参数的2范数项，以减少模型对训练集的过拟合，即提高模型的泛化能力。那么问题来了，为什么2范数可以提高模型的泛化能力呢？ 首先假设我们model的参数为向量W=[W1, W2, …, Wn], 这个w不仅是模型的参数，更是特征的权重！。换句话说，每个参数Wi，决定了每个特征对决定样本所属类别的重要程度。 这里我们举一个实际例子，假设参数w的维度为5，有以下两种情况： 令W1=W2=W3=W4=W5=2 令W1=10, W2=W3=W4=W5=0 由二范数公式：1和2相比，哪个的Reg更小呢？显然前者的值只有20，而后者的值高达100！虽然1和2的所有w的值加起来都等于10。而我们的目标是最小化loss函数，因此需要Reg也要小。由这个例子可以看出，如果我们有10张用于决定类别的票分给各个特征，那么给每个特征分两张票带来的回报要远大于把这10张票分给一个特征！（不患寡而患不均），因此二范数会削弱强特征，增强弱特征。（有点儿劫富济贫的意思） 那么这种方式有什么好处呢？ 比如，假设我们在做文本的情感分类任务（判断一段文本是正面还是负面），将每个词作为一个特征（出现该词代表1，否则为0）。可想而知，有一些词本身就带有很强的情感极性，比如“不好”、“惊喜”等。而大部分词是弱极性的，但是多个弱极性的词同时出现的时候就会产生很强的情感极性。比如“总体”“来说”“还是”“可以”在文本中同时出现后基本就奠定了这篇文本的总体极性是正面的，哪怕文本中出现了“很烂”这种强负面词。 因此在二范数的约束下，w这个随机变量的分布会趋向于方差u=0的高斯分布。当然，既然是高斯分布，有极少的特征值是特别大的，这些特征就是超强特征，比如“非常好”这个特征一旦出现，基本整个文本的情感极性就确定了，其它的弱特征是很难与之对抗的。所以最小化二范数会让随机变量的采样点组成的向量趋向于期望=0的高斯分布。 所以，如果没有二范数的约束，弱特征会被强特征“剥削”，最终训练完后各个特征的权重很有可能是这样的：这样会带来什么问题呢？导致模型过分依赖强特征。 首先，这样的model拿到测试集上，一旦某个样本没有任何强特征，就会导致这个样本的分类很随机。而如果有了二范数的约束，弱特征们就会发挥它们的作用。 其次，这样的model抗噪声能力非常差。一旦测试集中的某个样本出现了一个强特征，就会直接导致其进行分类，而完全不考虑其它弱特征的存在。而如果有了二范数的约束，弱特征们就会聚沙成塔，合力打倒那个强特征的噪声。那么是不是所有的ML任务加上二范数约束就一定好呢？答案当然是否定的。在一些ML任务中，尤其是一些结构化数据挖掘任务和特征意义模糊的机器学习任务（比如深度学习）中，特征分布本来就是若干强特征与噪声的组合，这时加上2范数约束反而会引入噪声，降低系统的抗噪性能，导致model的泛化能力更差。因此，使用二范数去提高机器学习model的泛化能力大部分情况下是没错的，但是也不要无脑使用哦，懂得意义后学会根据任务去感性与理性的分析才是正解。这里有一篇分析机器学习中正则化项L1和L2的直观理解，大家可以借鉴阅读。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习中的线性代数知识（上）]]></title>
    <url>%2F2018%2F11%2F08%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E7%9F%A5%E8%AF%86(%E4%B8%8A)%2F</url>
    <content type="text"><![CDATA[前言As all we know, 线性代数对于机器学习的重要性不言而喻。但纵观国内的教材和课程，大部分线性代数的讲解，一上来就堆满了各种定义和公式，从而导致我们知其然而不知其所以然，不利于我们深入理解机器学习的算法。因此，希望本篇博文能帮大家从另一个角度理解线性代数。但是注意，阅读本篇博文，最好已经有了线性代数的基础，比如要知道矩阵的形式，矩阵怎么相乘之类的基础知识。然后如果你虽然知道矩阵相乘怎么算，却并不知道它们有什么物理含义，那么相信本篇博文会让您有所收获。本系列博文分为上下两篇，上篇主要讲矩阵的本质，下篇主要讲向量的范数和矩阵分解。 矩阵的本质（一）1、矩阵本质上就是映射、变换比如，对于空间A中的一个三维点a(a1,a2,a3)，空间B的一个二维点b(b1,b2)，那么如何将a点映射成b点呢？只需构造一个维度为3*2的矩阵，利用矩阵乘法便可实现映射。矩阵的本质就是描述空间的映射。因此它可以看作一个函数，接受一个空间作为输入，输出一个映射后的新空间。 2、各种矩阵运算的本质是什么？ 加法：矩阵的加法就是映射规则的调整。矩阵A+矩阵B就可以看作，用B中各个元素的值去调整A这个映射规则。 乘法：矩阵的乘法表示线性映射的叠加。比如，我们已经知道矩阵A可以从X空间映射到Z空间，矩阵B可以从Z空间映射到Y空间，那么怎么从X空间映射到Y呢？答案就是矩阵乘法A*B。 逆运算：矩阵的逆运算就是映射的逆映射；这里我用一个NLP的例子来解释，比如在一个机器翻译问题中，源语言在X空间，目标语言在Y空间，我们通过训练知道了如何从X空间映射到Y空间，即通过训练得到了映射矩阵A。那么我们如何反向翻译呢？那就是求A的逆矩阵就可以了，完全无需再反向重新训练。 3、特殊矩阵的含义（从映射角度理解） 零矩阵就是将任何空间映射成原点的矩阵； 单位矩阵就是将任何空间映射成自己的矩阵； 对角矩阵就是将原空间的各个坐标轴进行不同尺度的拉伸或挤压后生成新空间的矩阵；比如任何一个二维空间乘上一个对角矩阵后，这个空间第一个维度（每一列表示一个维度）上的坐标轴会缩短为原来的一半，第二个维度的坐标轴会膨胀成原来的两倍！假设这个二维空间中的点(2,2)经过这个对角矩阵的映射后，会变成(1,4)。4、矩阵分块的意义矩阵分块对应的算法思想就是分治，一个最重要的应用就是并行化矩阵运算，这也是深度学习中最离不开的底层运算。设想一下，一个10000*10000维的超大矩阵再乘以另一个10000*20000维的超大矩阵，如果让一个单核CPU去计算的话，可能CPU会崩溃！我们知道，在深度学习任务中，由于参数非常多，数据维度也很高，因此进行大矩阵运算是家常便饭。同时，我们之所以喜欢用GPU去训练深度学习任务，一个很重要的原因是GPU的“核”很多，经常几百几千，甚至几万核，非常利于并行计算。因此，在深度学习中，通过将大矩阵分块，分成若干个小块，再将这些小块丢进GPU成千上万的核里并行计算，那么转瞬间就可以完成这个大矩阵的计算。矩阵的本质（二）1、从存储数据的角度矩阵A就是存储数据的作用了。矩阵的每一行表示空间中的一个样本点，所以A就表示二维空间里的三个样本点；如果将A这三个样本点丢给映射W，就得到了三个样本点在新空间上的新空间上的镜像点A x W =在神经网络中，每一层的输出经过权重矩阵，映射到下一层的输入过程，就是上述过程。2、特征值与特征向量的物理意义我们将矩阵W重新夸张一下W=那么这个映射就代表将第一维度压缩为原来的0.99倍（几乎无变化），将第二维度拉伸为原来的100倍（变化很大）。这个映射的作用对象很明显： 第一维度坐标轴。在二维空间中，第一维度坐标轴不就是x轴嘛，所以我们可以用单位向量(1, 0)表示； 第二维度坐标轴。同理，第二维度可表示为(0, 1);显而易见，W映射对每个作用对象的作用程度明显不一样：对第一维度作用小，对第二维度作用大；升华一下，上述的作用对象就是所谓的特征向量，“对某作用对象的作用程度” 就是特征值。因此，一个矩阵或一个线性映射，完全可以用它的全部特征值与其对应的特征值去描述！（这里指方阵）；而将矩阵分解为它的特征值与特征向量的过程被称为“特征分解”，特征矩阵分解是众多矩阵分解中的一种，因此原矩阵A就会被分解成几个目标矩阵了，这里的目标矩阵有两个，一个是由特征向量组成的，另一个是由特征值组成的；3、标量、向量、张量之间的联系 张量（tensor）在某些情况下，我们会讨论坐标超过两维的数组。一般地，一个数组中的元素分布在若干维坐标的规则网格中，我们将其称之为张量。使用粗体A来表示张量“A”。张量A中坐标为(i,j,k)的元素记作A(i,j,k)。 关系标量是0阶张量，向量是一阶张量。 举例：标量就是知道棍子的长度，但是不知道棍子指向哪儿；向量就是不但知道棍子的长度，还知道棍子指向前面还是后面；张量就是不但知道棍子的长度，也知道棍子指向前面还是后面，还能知道这棍子向上/下和左/右偏转了多少。4、张量与矩阵的区别 从代数角度讲，矩阵是向量的推广。向量可以看成是一维的“表格”（即分量按照顺序排成一列），矩阵是二维的“表格”（分量按照纵横位置排列），那么n阶张量就是所谓的n维的“表格”，故可以将矩阵视为二阶张量。张量的严格定义是利用线性映射来描述的。 从几何角度讲，矩阵是一个真正的几何量，也就是说，它是一个不随参照系的坐标变换而变化的东西。向量也具有这种特性。 矩阵是张量的一个切片。 张量是矩阵基础上更高维度的抽象，简而言之，张量囊括一切。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法的评价指标]]></title>
    <url>%2F2018%2F11%2F06%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%2F</url>
    <content type="text"><![CDATA[前言机器学习的目标是使得我们模型的泛化能力足够强，因此我们需要有衡量模型泛化能力的评价标准。对不同种类的机器学习问题，评价标准不一样。回归问题常用均方误差(MSE)、绝对误差(MAE)等评价指标，分类问题评价指标则较多，如下图所示。本文主要讲解分类问题的评价指标。 二分类问题中常用的概念首先解释几个二分类问题中常用的概念：True Positive, False Positive, True Negative, False Negative。它们是根据真实类别与预测类别的组合来区分的。假设有一批测试样本，只包含正例和反例两种类别。则有： 预测值正例，记为P(positive) 预测值为反例，记为N(Negative) 预测值与真实值相同，记为T(True) 预测值与真实值相反，记为F(False)上图左半部分表示预测为正例，右半部分预测类别为反例；样本中的真实正例类别在上半部分，下半部分为真实的反例。 TP: 预测类别为正，真实类别也为正例； FP：预测类别为正例，真实类别为反例； FN：预测为反例，真实类别为正例； TN：预测为反例，真实类别也为正例； 精确率（precision）精确率是针对我们模型预测的结果而言的，它表示的是预测为正的样本中有多少是真正的正样本；故分子为TP，而预测为正有两种可能：真正(TP)和假正(FP)，即为分母。 召回率（recall）召回率是针对原始样本而言的，表示样本中的正例有多少被预测正确了，故分母应表示所有的原始样本； F1值F1值是精确率和召回率的调和均值，当P和R值都高时，F1值也会高。 ROC、AUCROC全称是“受试者工作特征”（Receiver Operating Characteristic）。对于计算ROC，有三个重要概念：TPR，FPR，截断点。TPR即 True Positive Rate，表示真实的正例中，被预测正确的比例，即TPR = TP/(TP+FN);FPR即 False Positive Rate, 表示真实的反例中，被预测正确的比例，即FPR = FP/(TN+FP);很多ML算法是为test样本产生一个概率预测，然后将这个概率预测值与一个分类阈值(threhold)进行比较，若大于阈值则分为正类，反之为负类。这里的阈值就是截断点，比如对算出测试样本A为正类的概率为0.3，低于截断点0.5，于是将A分为负类。截断点取不同的值，TPR和FPR的计算结果也不同。将截断点不同取值下对应的TPR和FPR结果作图得到的曲线，就是ROC曲线。横轴是FPR，纵轴是TPR。若比较两个算法的优劣，可以看哪个曲线能把另一个完全包住；或者计算曲线的面积，即AUC的值。ROC经常用于样本类别不均衡的二分类问题。 sklearn实现roc、auc123from sklearn.metrics import roc_curve, aucfpr, tpr, th = roc_curve(test_Y, pred_y) # 得到一组fpr、tpr的值roc_auc = auc(fpr, tpr) # 计算auc的值]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
</search>
